{
  
    
        "post0": {
            "title": "The building blocks of DL 2",
            "content": "! git clone https://github.com/apolanco3225/Medical-MNIST-Classification.git ! mv Medical-MNIST-Classification/resized/ ./medical_mnist ! rm -rf Medical-MNIST-Classification . Cloning into &#39;Medical-MNIST-Classification&#39;... remote: Enumerating objects: 58532, done. remote: Total 58532 (delta 0), reused 0 (delta 0), pack-reused 58532 Receiving objects: 100% (58532/58532), 77.86 MiB | 20.76 MiB/s, done. Resolving deltas: 100% (506/506), done. Checking connectivity... done. Checking out files: 100% (58959/58959), done. . from pathlib import Path PATH = Path(&quot;medical_mnist/&quot;) . We have much more powerful tools now, so we will deal with all 6 classes now, but first need to prepare data: . all data needs to be numerical | it needs to be in arrays | it needs to be labeled | . classes = !ls {PATH} classes . [&#39;AbdomenCT&#39;, &#39;BreastMRI&#39;, &#39;CXR&#39;, &#39;ChestCT&#39;, &#39;Hand&#39;, &#39;HeadCT&#39;] . Prepare data . images = {} for cls in classes: images[cls] = !ls {PATH/cls} . from PIL import Image . # ToTensor converts images to tensorts from torchvision.transforms import ToTensor . import torch image_tensors = {} for cls in classes: image_tensors[cls] = torch.stack( [ ToTensor()( Image.open(path) ).view(-1, 64 * 64).squeeze().float()/255 for path in (PATH/cls).iterdir()] ) . for cls in classes: class_shape = image_tensors[cls].shape print(f&quot;{cls} has {class_shape[0]} images of a size {class_shape[1:]}&quot;) . AbdomenCT has 10000 images of a size torch.Size([4096]) BreastMRI has 8954 images of a size torch.Size([4096]) CXR has 10000 images of a size torch.Size([4096]) ChestCT has 10000 images of a size torch.Size([4096]) Hand has 10000 images of a size torch.Size([4096]) HeadCT has 10000 images of a size torch.Size([4096]) . x_train = torch.cat([image_tensors[cls] for cls in classes], dim=0) y_train = torch.cat([torch.tensor([index] * image_tensors[cls].shape[0]) for index, cls in enumerate(classes)]) . shuffle the dataset. This is important as if we don&#39;t do this, images from the classes that we train first will be effectively not as &quot;fresh&quot; in the memmory of the network . permutations = torch.randperm(x_train.shape[0]) . x_train = x_train[permutations] y_train = y_train[permutations] . create validation set that is 20% of the training set - this is important to asses the performance of the model. Validation set doesn&#39;t take part in training, so model is not biased towards those images (it cannot remember those exact images from training). This is essential to see how well model generalizes on examples it has not seen. . valid_pct = 0.2 valid_index = int(x_train.shape[0] * valid_pct) valid_index . 11790 . # we take out first 20% of examples from the training set x_valid = x_train[:valid_index] y_valid = y_train[:valid_index] x_train = x_train[valid_index:] y_train = y_train[valid_index:] . x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . (torch.Size([47164, 4096]), torch.Size([47164]), torch.Size([11790, 4096]), torch.Size([11790])) .",
            "url": "https://ml4med.github.io/blog/medicalnist/mnist/basic/2020/07/23/building-blocks-of-dl-linear_model_on_medicalnist-Copy1.html",
            "relUrl": "/medicalnist/mnist/basic/2020/07/23/building-blocks-of-dl-linear_model_on_medicalnist-Copy1.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "The building blocks of DL 1",
            "content": "Deep learning is so simple that it is hard to believe. Especially with how much magical it seems at first. At it&#39;s core is a piece of code that allows, one step at a time, to be just a bit closer to the solution. You can think of it like the way of finding the top of the mountain: . . Vocabulary: . step - single step of the optimization algorithm that makes parameters of a function just a bit better | gradient - the slope of the loss function, that we want to minimize | learning rate - how fast we adjust parameters based on the gradient | . Task: Find minimum for $f: a x^2 + b$ with Gradient Descent . We have single variable here: $x$. With Gradient Descent we&#39;ll find the value where $f$ is minimal. . In short, gradient descent is an algorithm that walks down the slope - no more no less. . An example function $f$ in python looks like this: . def f(x): return 1.2 * x**2 + 4 . To compute gradient at where we are now, we will use PyTorch magic: . import torch x = torch.tensor(-3.).requires_grad_() . To walk along the function, we will change $x$ by the fraction of the gradient (gradient * learning_rate) . learning_rate = 0.2 . and this is gradient descent algorightm in code: . # single step of the gradient descent algorithm def step(x, f): # compute y at a given value of x y = f(x) # the backward function of PyTorch tensor computes gradients y.backward() # change value of x in a direction where function decreases # (don&#39;t bother about x.data and x.grad = None - it is just necessary boilerplate) x.data = x - learning_rate * x.grad.data x.grad = None return x . gather x-es and y-s for 10 iterations . xypairs = [] for i in range(10): xypairs.append([x.data, f(x)]) print(f&quot;f({x:.2}) = {f(x):.2}&quot;) x = step(x, f) . f(-3.0) = 1.5e+01 f(-1.6) = 6.9 f(-0.81) = 4.8 f(-0.42) = 4.2 f(-0.22) = 4.1 f(-0.11) = 4.0 f(-0.059) = 4.0 f(-0.031) = 4.0 f(-0.016) = 4.0 f(-0.0083) = 4.0 . &lt;/input&gt; Once Loop Reflect And believe it or not, this is the building block of each the Neural Net - gradient descent algirithm. Stay tuned for the next post where we will use it on a MedicalNist dataset. .",
            "url": "https://ml4med.github.io/blog/basic/sgd/2020/07/20/building-blocks-of-dl-sgd.html",
            "relUrl": "/basic/sgd/2020/07/20/building-blocks-of-dl-sgd.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "MedicalNIST the most basic prediction technique",
            "content": "To start off the blog, I&#39;ve chosen the most basic example I could come up with: . Medical imaging categorization based on comparison between the &quot;statistically average&quot; image from a category and a set of test images. . This will be used to build upon using more advanced techniques, so stay tuned! . But first let&#39;s download the data . ! rm -rf ./medical_mnist ! git clone https://github.com/apolanco3225/Medical-MNIST-Classification.git ! mv Medical-MNIST-Classification/resized/ ./medical_mnist ! rm -rf Medical-MNIST-Classification . Cloning into &#39;Medical-MNIST-Classification&#39;... remote: Enumerating objects: 58532, done. remote: Total 58532 (delta 0), reused 0 (delta 0), pack-reused 58532 Receiving objects: 100% (58532/58532), 77.86 MiB | 4.39 MiB/s, done. Resolving deltas: 100% (506/506), done. Checking connectivity... done. Checking out files: 100% (58959/58959), done. . install useful libraries . ## run this if you don&#39;t have pytorch and fastai2 installed # !pip install torch torchvision . data will be downloaded to medical_mnist folder . from pathlib import Path data = Path(&#39;medical_mnist&#39;) list(data.iterdir()) . [PosixPath(&#39;medical_mnist/AbdomenCT&#39;), PosixPath(&#39;medical_mnist/BreastMRI&#39;), PosixPath(&#39;medical_mnist/CXR&#39;), PosixPath(&#39;medical_mnist/ChestCT&#39;), PosixPath(&#39;medical_mnist/Hand&#39;), PosixPath(&#39;medical_mnist/HeadCT&#39;)] . let&#39;s see what we have here... as this is the most basic technique, let&#39;s pick the images that look the most different from each other . import matplotlib.pyplot as plt from PIL import Image for d in data.iterdir(): print(d) plt.imshow(Image.open(list(d.iterdir())[0])) plt.show() . medical_mnist/AbdomenCT . medical_mnist/BreastMRI . medical_mnist/CXR . medical_mnist/ChestCT . medical_mnist/Hand . medical_mnist/HeadCT . load the data into tensors . import torch from torchvision.transforms import ToTensor stacked_cxrs = torch.stack([ToTensor()(Image.open(path)).float()/255 for path in (data/&#39;CXR&#39;).iterdir()]) stacked_heads = torch.stack([ToTensor()(Image.open(path)).float()/255 for path in (data/&#39;HeadCT&#39;).iterdir()]) . as a good practice, let&#39;s look at the first image, so see if we did it correctly . plt.imshow(stacked_cxrs[0][0]) . &lt;matplotlib.image.AxesImage at 0x7f2198995610&gt; . now, let&#39;s build &quot;ideal&quot; image for each of the category. This ideal image is just a mean for each pixel across all the images . mean_cxrs = stacked_cxrs.mean(0) plt.imshow(mean_cxrs[0]) . &lt;matplotlib.image.AxesImage at 0x7f21717d1590&gt; . mean_headct = stacked_heads.mean(0) plt.imshow(mean_headct[0]) . &lt;matplotlib.image.AxesImage at 0x7f211f6b6a50&gt; . now we can see how much example image differs from the ideals: . import torch.nn.functional as F . F.mse_loss(stacked_cxrs[0], mean_cxrs).sqrt() . tensor(0.0010) . F.mse_loss(stacked_cxrs[0], mean_headct).sqrt() . tensor(0.0016) . looks like that one was a CXR indeed - L2 loss between ideal image from CXR category (mean_cxrs) was lower . so let&#39;s build a simple classifier function, that predicts whether image is a headct or not . def is_headct(img_tensor): if F.mse_loss(img_tensor, mean_cxrs) &gt; F.mse_loss(img_tensor, mean_headct): return True else: return False . now we test the classifier . cxrs_preds = torch.tensor([not is_headct(stacked_cxrs[i]) for i in range(stacked_cxrs.shape[0])]) cxrs_accuracy = cxrs_preds.sum().float() / cxrs_preds.shape[0] print(f&#39;Accuracy on CXRs: {round( (cxrs_accuracy).item() * 100, 2)}%&#39;) . Accuracy on CXRs: 99.15% . head_preds = torch.tensor([is_headct(stacked_heads[i]) for i in range(stacked_heads.shape[0])]) head_accuracy = head_preds.sum().float() / head_preds.shape[0] print(f&#39;Accuracy on HeadCTs: {round( (head_accuracy).item() * 100, 2)}%&#39;) . Accuracy on HeadCTs: 100.0% . ... of course: . those classes were the most distinguishable from each other and | we didn&#39;t split into train and test sets here so results are biased (as each image we predict was used to figure out the &quot;ideal&quot; image) | but this was a nice start of this blog :) .",
            "url": "https://ml4med.github.io/blog/medicalnist/mnist/basic/2020/06/19/fist_post.html",
            "relUrl": "/medicalnist/mnist/basic/2020/06/19/fist_post.html",
            "date": " • Jun 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, . My name is Michał Pawłowski and I am a data engineering consultant for a pharma company. . Here I will post my learnings and experiments with ML and AI in the field of medicine. . If you want to find our a bit more about me, see linkedin .",
          "url": "https://ml4med.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ml4med.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}