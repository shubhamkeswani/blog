{
  
    
        "post0": {
            "title": "The building blocks of DL 1",
            "content": "Deep learning is so simple that it is hard to believe. Especially with how much magical it seems at first. At it&#39;s core is a piece of code that allows, one step at a time, to be just a bit closer to the solution. You can think of it like the way of finding the top of the mountain: . . Vocabulary: . step - single step of the optimization algorithm that makes parameters of a function just a bit better | gradient - the slope of the loss function, that we want to minimize | learning rate - how fast we adjust parameters based on the gradient | . Task: Find minimum for $f: a x^2 + b$ with Gradient Descent . We have single variable here: $x$. With Gradient Descent we&#39;ll find the value where $f$ is minimal. . In short, gradient descent is an algorithm that walks down the slope - no more no less. . An example function $f$ in python looks like this: . def f(x): return 1.2 * x**2 + 4 . To compute gradient at where we are now, we will use PyTorch magic: . import torch x = torch.tensor(-3.).requires_grad_() . To walk along the function, we will change $x$ by the fraction of the gradient (gradient * learning_rate) . learning_rate = 0.2 . and this is gradient descent algorightm in code: . # single step of the gradient descent algorithm def step(x, f): # compute y at a given value of x y = f(x) # the backward function of PyTorch tensor computes gradients y.backward() # change value of x in a direction where function decreases # (don&#39;t bother about x.data and x.grad = None - it is just necessary boilerplate) x.data = x - learning_rate * x.grad.data x.grad = None return x . gather x-es and y-s for 10 iterations . xypairs = [] for i in range(10): xypairs.append([x.data, f(x)]) print(f&quot;f({x:.2}) = {f(x):.2}&quot;) x = step(x, f) . f(-3.0) = 1.5e+01 f(-1.6) = 6.9 f(-0.81) = 4.8 f(-0.42) = 4.2 f(-0.22) = 4.1 f(-0.11) = 4.0 f(-0.059) = 4.0 f(-0.031) = 4.0 f(-0.016) = 4.0 f(-0.0083) = 4.0 . &lt;/input&gt; Once Loop Reflect And believe it or not, this is the building block of each the Neural Net - gradient descent algirithm. Stay tuned for the next post where we will use it on a MedicalNist dataset. .",
            "url": "https://ml4med.github.io/blog/basic/sgd/2020/07/20/building-blocks-of-dl-sgd.html",
            "relUrl": "/basic/sgd/2020/07/20/building-blocks-of-dl-sgd.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "MedicalNIST the most basic prediction technique",
            "content": "To start off the blog, I&#39;ve chosen the most basic example I could come up with: . Medical imaging categorization based on comparison between the &quot;statistically average&quot; image from a category and a set of test images. . This will be used to build upon using more advanced techniques, so stay tuned! . But first let&#39;s download the data . ! rm -rf ./medical_mnist ! git clone https://github.com/apolanco3225/Medical-MNIST-Classification.git ! mv Medical-MNIST-Classification/resized/ ./medical_mnist ! rm -rf Medical-MNIST-Classification . Cloning into &#39;Medical-MNIST-Classification&#39;... remote: Enumerating objects: 58532, done. remote: Total 58532 (delta 0), reused 0 (delta 0), pack-reused 58532 Receiving objects: 100% (58532/58532), 77.86 MiB | 4.39 MiB/s, done. Resolving deltas: 100% (506/506), done. Checking connectivity... done. Checking out files: 100% (58959/58959), done. . install useful libraries . ## run this if you don&#39;t have pytorch and fastai2 installed # !pip install torch torchvision . data will be downloaded to medical_mnist folder . from pathlib import Path data = Path(&#39;medical_mnist&#39;) list(data.iterdir()) . [PosixPath(&#39;medical_mnist/AbdomenCT&#39;), PosixPath(&#39;medical_mnist/BreastMRI&#39;), PosixPath(&#39;medical_mnist/CXR&#39;), PosixPath(&#39;medical_mnist/ChestCT&#39;), PosixPath(&#39;medical_mnist/Hand&#39;), PosixPath(&#39;medical_mnist/HeadCT&#39;)] . let&#39;s see what we have here... as this is the most basic technique, let&#39;s pick the images that look the most different from each other . import matplotlib.pyplot as plt from PIL import Image for d in data.iterdir(): print(d) plt.imshow(Image.open(list(d.iterdir())[0])) plt.show() . medical_mnist/AbdomenCT . medical_mnist/BreastMRI . medical_mnist/CXR . medical_mnist/ChestCT . medical_mnist/Hand . medical_mnist/HeadCT . load the data into tensors . import torch from torchvision.transforms import ToTensor stacked_cxrs = torch.stack([ToTensor()(Image.open(path)).float()/255 for path in (data/&#39;CXR&#39;).iterdir()]) stacked_heads = torch.stack([ToTensor()(Image.open(path)).float()/255 for path in (data/&#39;HeadCT&#39;).iterdir()]) . as a good practice, let&#39;s look at the first image, so see if we did it correctly . plt.imshow(stacked_cxrs[0][0]) . &lt;matplotlib.image.AxesImage at 0x7f2198995610&gt; . now, let&#39;s build &quot;ideal&quot; image for each of the category. This ideal image is just a mean for each pixel across all the images . mean_cxrs = stacked_cxrs.mean(0) plt.imshow(mean_cxrs[0]) . &lt;matplotlib.image.AxesImage at 0x7f21717d1590&gt; . mean_headct = stacked_heads.mean(0) plt.imshow(mean_headct[0]) . &lt;matplotlib.image.AxesImage at 0x7f211f6b6a50&gt; . now we can see how much example image differs from the ideals: . import torch.nn.functional as F . F.mse_loss(stacked_cxrs[0], mean_cxrs).sqrt() . tensor(0.0010) . F.mse_loss(stacked_cxrs[0], mean_headct).sqrt() . tensor(0.0016) . looks like that one was a CXR indeed - L2 loss between ideal image from CXR category (mean_cxrs) was lower . so let&#39;s build a simple classifier function, that predicts whether image is a headct or not . def is_headct(img_tensor): if F.mse_loss(img_tensor, mean_cxrs) &gt; F.mse_loss(img_tensor, mean_headct): return True else: return False . now we test the classifier . cxrs_preds = torch.tensor([not is_headct(stacked_cxrs[i]) for i in range(stacked_cxrs.shape[0])]) cxrs_accuracy = cxrs_preds.sum().float() / cxrs_preds.shape[0] print(f&#39;Accuracy on CXRs: {round( (cxrs_accuracy).item() * 100, 2)}%&#39;) . Accuracy on CXRs: 99.15% . head_preds = torch.tensor([is_headct(stacked_heads[i]) for i in range(stacked_heads.shape[0])]) head_accuracy = head_preds.sum().float() / head_preds.shape[0] print(f&#39;Accuracy on HeadCTs: {round( (head_accuracy).item() * 100, 2)}%&#39;) . Accuracy on HeadCTs: 100.0% . ... of course: . those classes were the most distinguishable from each other and | we didn&#39;t split into train and test sets here so results are biased (as each image we predict was used to figure out the &quot;ideal&quot; image) | but this was a nice start of this blog :) .",
            "url": "https://ml4med.github.io/blog/medicalnist/mnist/basic/2020/06/19/fist_post.html",
            "relUrl": "/medicalnist/mnist/basic/2020/06/19/fist_post.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The building blocks of DL 2",
            "content": "In the previous post I&#39;ve explained what is the most important thing in neural networks - technique that allows us to incrementally find minimum of a function. This is called Gradient Descent algorithm! . In this post I will build on this concept and show you how to create a basic linear model to predict what is on a medical image! . Download data . As in the first post showing how to build medical image recognition with pure statistics, we need to download data first. For some basic description of how the data looks like, see that first post . ! git clone https://github.com/apolanco3225/Medical-MNIST-Classification.git ! mv Medical-MNIST-Classification/resized/ ./medical_mnist ! rm -rf Medical-MNIST-Classification . Cloning into &#39;Medical-MNIST-Classification&#39;... remote: Enumerating objects: 58532, done. remote: Total 58532 (delta 0), reused 0 (delta 0), pack-reused 58532 Receiving objects: 100% (58532/58532), 77.86 MiB | 2.46 MiB/s, done. Resolving deltas: 100% (506/506), done. Checking connectivity... done. Checking out files: 100% (58959/58959), done. . from pathlib import Path PATH = Path(&quot;medical_mnist/&quot;) . We have much more powerful tools now, so we will deal with all 6 classes now, but first need to prepare data: . all data needs to be numerical | it needs to be in arrays | it needs to be labeled | . classes = !ls {PATH} classes . [&#39;AbdomenCT&#39;, &#39;BreastMRI&#39;, &#39;CXR&#39;, &#39;ChestCT&#39;, &#39;Hand&#39;, &#39;HeadCT&#39;] . images = {} for cls in classes: images[cls] = !ls {PATH/cls} . from PIL import Image . # ToTensor converts images to tensorts from torchvision.transforms import ToTensor . import torch image_tensors = {} for cls in classes: image_tensors[cls] = torch.stack([ToTensor()(Image.open(path)).view(-1, 64 * 64).squeeze().float()/255 for path in (PATH/cls).iterdir()]) . for cls in classes: class_shape = image_tensors[cls].shape print(f&quot;{cls} has {class_shape[0]} images of a size {class_shape[1:]}&quot;) . AbdomenCT has 10000 images of a size torch.Size([4096]) BreastMRI has 8954 images of a size torch.Size([4096]) CXR has 10000 images of a size torch.Size([4096]) ChestCT has 10000 images of a size torch.Size([4096]) Hand has 10000 images of a size torch.Size([4096]) HeadCT has 10000 images of a size torch.Size([4096]) . x_train = torch.cat([image_tensors[cls] for cls in classes], dim=0) y_train = torch.cat([torch.tensor([index] * image_tensors[cls].shape[0]) for index, cls in enumerate(classes)]) . shuffle the dataset. This is important as if we don&#39;t do this, images from the classes that we train first will be effectively not as &quot;fresh&quot; in the memmory of the network . permutations = torch.randperm(x_train.shape[0]) . x_train = x_train[permutations] y_train = y_train[permutations] . create validation set that is 20% of the training set - this is important to asses the performance of the model. Validation set doesn&#39;t take part in training, so model is not biased towards those images (it cannot remember those exact images from training). This is essential to see how well model generalizes on examples it has not seen. . valid_pct = 0.2 valid_index = int(x_train.shape[0] * valid_pct) valid_index . 11790 . # we take out first 20% of examples from the training set x_valid = x_train[:valid_index] y_valid = y_train[:valid_index] x_train = x_train[valid_index:] y_train = y_train[valid_index:] . x_train.shape, y_train.shape, x_valid.shape, y_valid.shape . (torch.Size([47164, 4096]), torch.Size([47164]), torch.Size([11790, 4096]), torch.Size([11790])) . Now we need to build the model. Let&#39;s use the most basic building blocks of the neural network - linear layer (linear_layer function) and nonlinearity (softmax function). . # it normalizes all 10 classes so we can treat each class prediction as probability that add up to 1.0 def softmax(x): return x - x.exp().sum(-1).unsqueeze(-1) . def linear_layer(x): return x @ weights + bias . the model is a simple function composition (results of linear layer and fed into nonlinear layer (softmax) . def model(x): return softmax(linear_layer(x)) . For the Gradient Descent to work we also need to specify the loss function - this is crucial, as this is the function on which we compute gradients for our parameters. Just a quick recap: Gradient Descent algorithm finds out the values to change function parameters so the function values decreese. . In your case we minimize loss_func. Parameters of this function (passed in a form of preds) are in the model: wegiths and bias. Gradient Descent will give us values to change each of those parameters so we minimize the loss_func . def loss_func(preds, targets): return -preds[range(targets.shape[0]), targets].mean() def accuracy(preds, targets): return (torch.argmax(preds, dim=-1) == targets).float().mean() . And here is the Gradient Descent loop - see comments in the code for details . # number of training examples n = x_train.shape[0] # batch size - this is necessary as we won&#39;t be able to fit all # the examples into the memmory, so we need to do the computations in batches bs = 64 # how many epochs to train for epochs = 15 weights = torch.zeros((64 * 64, 10), requires_grad=True) # define weights matrix bias = torch.zeros(10, requires_grad=True) # and bias term # in each of those epochs algorithm sees all the images. So in this case # we see all the images 15 times for epoch in range(epochs): # here is the loop for batches: in each batch we: # - see 64 images # - compute predictions based on the model # - compute the loss # - compute gradients and update parameters (wegiths and bias) for i in range((n - 1) // bs + 1): # select images for this batch start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] # compute predictions preds = model(xb) # compute loss loss = loss_func(preds, yb) # compute gradients (this is done for us by PyTorch with this backwards function!) loss.backward() # this block is necessary, so computations we do below, are not taken into account when # computing next gradients with torch.no_grad(): # update parameters weights -= weights.grad bias -= bias.grad # zero out the gradients so they are ready for the next batch (otherwise they accumulate values) weights.grad.zero_() bias.grad.zero_() # eventually after each epoch (seeing all the images) we print out how we did print(f&quot;Epoch {epoch} accuracy: {accuracy(model(x_valid),y_valid)}, loss: {loss_func(model(x_valid), y_valid)}&quot;) . Epoch 0 accuracy: 0.6099236607551575, loss: 2.54538631439209 Epoch 1 accuracy: 0.8692111968994141, loss: 2.362863779067993 Epoch 2 accuracy: 0.9257845878601074, loss: 2.2319068908691406 Epoch 3 accuracy: 0.9413909912109375, loss: 2.133662223815918 Epoch 4 accuracy: 0.9471586346626282, loss: 2.057248115539551 Epoch 5 accuracy: 0.9513146877288818, loss: 1.9960107803344727 Epoch 6 accuracy: 0.9527565836906433, loss: 1.9457216262817383 Epoch 7 accuracy: 0.953350305557251, loss: 1.90358567237854 Epoch 8 accuracy: 0.9547922015190125, loss: 1.8676912784576416 Epoch 9 accuracy: 0.9554707407951355, loss: 1.8366881608963013 Epoch 10 accuracy: 0.9565733671188354, loss: 1.8095965385437012 Epoch 11 accuracy: 0.956912636756897, loss: 1.7856864929199219 Epoch 12 accuracy: 0.9574215412139893, loss: 1.7644044160842896 Epoch 13 accuracy: 0.9576759934425354, loss: 1.745321273803711 Epoch 14 accuracy: 0.9577608108520508, loss: 1.7281001806259155 . A simple Neural Network . This will be the simplest neural network that meets criteria of universal approximation theorem, so in theory it can approximate any function providing we introduce enough parameters. .",
            "url": "https://ml4med.github.io/blog/medicalnist/mnist/basic/2020/06/19/building-blocks-of-dl-linear_model_on_medicalnist.html",
            "relUrl": "/medicalnist/mnist/basic/2020/06/19/building-blocks-of-dl-linear_model_on_medicalnist.html",
            "date": " • Jun 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, . My name is Michał Pawłowski and I am a data engineering consultant for a pharma company. . Here I will post my learnings and experiments with ML and AI in the field of medicine. . If you want to find our a bit more about me, see linkedin .",
          "url": "https://ml4med.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ml4med.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}