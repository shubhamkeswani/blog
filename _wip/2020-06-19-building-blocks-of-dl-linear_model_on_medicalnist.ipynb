{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The building blocks of DL 2: linear layer with activation function\n",
    "\n",
    "In the previous post I've explained what is the most important thing in neural networks - technique that allows us to incrementally find minimum of a function. This is called Gradient Descent algorithm!\n",
    "\n",
    "In this post I will build on this concept and show you how to create a basic linear model to predict what is on a medical image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "As in the first post showing how to build medical image recognition with pure statistics, we need to download data first. For some basic description of how the data looks like, see that first post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Medical-MNIST-Classification'...\n",
      "remote: Enumerating objects: 58532, done.\u001b[K\n",
      "remote: Total 58532 (delta 0), reused 0 (delta 0), pack-reused 58532\u001b[K\n",
      "Receiving objects: 100% (58532/58532), 77.86 MiB | 5.34 MiB/s, done.\n",
      "Resolving deltas: 100% (506/506), done.\n",
      "Checking connectivity... done.\n",
      "Checking out files: 100% (58959/58959), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/apolanco3225/Medical-MNIST-Classification.git\n",
    "! mv Medical-MNIST-Classification/resized/ ./medical_mnist\n",
    "! rm -rf Medical-MNIST-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH = Path(\"medical_mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have much more powerful tools now, so we will deal with all 6 classes now, but first need to prepare data:\n",
    " - all data needs to be numerical\n",
    " - it needs to be in arrays\n",
    " - it needs to be labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = !ls {PATH}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "for cls in classes:\n",
    "    images[cls] = !ls {PATH/cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToTensor converts images to tensorts\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image_tensors = {}\n",
    "\n",
    "for cls in classes:\n",
    "    image_tensors[cls] = torch.stack([ToTensor()(Image.open(path)).view(-1, 64 * 64).squeeze().float()/255 for path in (PATH/cls).iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbdomenCT has 10000 images of a size torch.Size([4096])\n",
      "BreastMRI has 8954 images of a size torch.Size([4096])\n",
      "CXR has 10000 images of a size torch.Size([4096])\n",
      "ChestCT has 10000 images of a size torch.Size([4096])\n",
      "Hand has 10000 images of a size torch.Size([4096])\n",
      "HeadCT has 10000 images of a size torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    class_shape = image_tensors[cls].shape\n",
    "    print(f\"{cls} has {class_shape[0]} images of a size {class_shape[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.cat([image_tensors[cls] for cls in classes], dim=0)\n",
    "y_train = torch.cat([torch.tensor([index] * image_tensors[cls].shape[0]) for index, cls in enumerate(classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = torch.randperm(x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[permutations]\n",
    "y_train = y_train[permutations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create validation set that is 20% of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11790"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pct = 0.2\n",
    "valid_index = int(x_train.shape[0] * valid_pct)\n",
    "valid_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = x_train[:valid_index]\n",
    "y_valid = y_train[:valid_index]\n",
    "x_train = x_train[valid_index:]\n",
    "y_train = y_train[valid_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47164, 4096]), torch.Size([47164]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it normalizes all 10 classes so we can treat each class prediction as probability that add up to 1.0\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return log_softmax(x @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(preds, targets):\n",
    "    return -preds[range(targets.shape[0]), targets].mean()\n",
    "\n",
    "def accuracy(preds, targets):\n",
    "    return (torch.argmax(preds, dim=-1) == targets).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.47073790431022644, loss: 1.65959894657135\n",
      "Epoch 1 accuracy: 0.6564037203788757, loss: 1.5399587154388428\n",
      "Epoch 2 accuracy: 0.9083121418952942, loss: 1.4410401582717896\n",
      "Epoch 3 accuracy: 0.9347752332687378, loss: 1.3571207523345947\n",
      "Epoch 4 accuracy: 0.9279050230979919, loss: 1.2848182916641235\n",
      "Epoch 5 accuracy: 0.9279050230979919, loss: 1.221755027770996\n",
      "Epoch 6 accuracy: 0.9306191802024841, loss: 1.1661864519119263\n",
      "Epoch 7 accuracy: 0.9348600506782532, loss: 1.116800308227539\n",
      "Epoch 8 accuracy: 0.9382527470588684, loss: 1.0725876092910767\n",
      "Epoch 9 accuracy: 0.9405428171157837, loss: 1.0327564477920532\n",
      "Epoch 10 accuracy: 0.944614052772522, loss: 0.9966747760772705\n",
      "Epoch 11 accuracy: 0.9467345476150513, loss: 0.9638299345970154\n",
      "Epoch 12 accuracy: 0.9485157132148743, loss: 0.9338014125823975\n",
      "Epoch 13 accuracy: 0.9498727917671204, loss: 0.9062393307685852\n",
      "Epoch 14 accuracy: 0.9508057832717896, loss: 0.8808506727218628\n",
      "Epoch 15 accuracy: 0.9516539573669434, loss: 0.8573866486549377\n",
      "Epoch 16 accuracy: 0.9528414011001587, loss: 0.8356353640556335\n",
      "Epoch 17 accuracy: 0.9536895751953125, loss: 0.8154152631759644\n",
      "Epoch 18 accuracy: 0.954028844833374, loss: 0.7965691685676575\n",
      "Epoch 19 accuracy: 0.9545377492904663, loss: 0.778960108757019\n",
      "Epoch 20 accuracy: 0.9548770189285278, loss: 0.7624692916870117\n",
      "Epoch 21 accuracy: 0.9553011059761047, loss: 0.7469920516014099\n",
      "Epoch 22 accuracy: 0.9553859233856201, loss: 0.7324363589286804\n",
      "Epoch 23 accuracy: 0.9560644626617432, loss: 0.7187210917472839\n",
      "Epoch 24 accuracy: 0.9558948278427124, loss: 0.7057737112045288\n",
      "Epoch 25 accuracy: 0.9563189148902893, loss: 0.693530261516571\n",
      "Epoch 26 accuracy: 0.9566581845283508, loss: 0.6819331645965576\n",
      "Epoch 27 accuracy: 0.9568278193473816, loss: 0.6709313988685608\n",
      "Epoch 28 accuracy: 0.956912636756897, loss: 0.6604782342910767\n",
      "Epoch 29 accuracy: 0.9572519063949585, loss: 0.6505327224731445\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "n  = x_train.shape[0]\n",
    "bs = 64\n",
    "lr = 0.5  # learning rate\n",
    "epochs = 30  # how many epochs to train for\n",
    "\n",
    "weights = torch.zeros((64 * 64, 10), requires_grad=True)\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "xb, yb = None, None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "    print(f\"Epoch {epoch} accuracy: {accuracy(model(x_valid),y_valid)}, loss: {loss_func(model(x_valid), y_valid)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Neural Network\n",
    "\n",
    "This will be the simplest neural network that meets criteria of [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), so in theory it can approximate any function providing we introduce enough parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
